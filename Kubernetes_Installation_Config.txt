#Kube-Master(Controller)  - VM 
#	Kube-Worker1
#	Kube-Worker2 

#https://kubernetes.io/docs/setup/
#https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
	
#Add Port: 0 - 65535
#Allow All Traffic for Demo!
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

sudo -i
apt update -y

#Set the appropriate hostname for each machine.
#-----------------------------------------------
sudo hostnamectl set-hostname "k-master"
exec bash

sudo vi /etc/hosts
172.31.14.67 k-master
#Remember to replace the IPs with those of your systems.
#And only the worker node:
#--------------------------
sudo hostnamectl set-hostname "k-worker1"
exec bash

sudo vi /etc/hosts
172.31.10.44 k-worker1
#Remember to replace the IPs with those of your systems.

sudo hostnamectl set-hostname "k-worker2"
exec bash

sudo vi /etc/hosts
172.31.1.230 k-worker2

#To allow kubelet to work properly, we need to disable swap on both machines.
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

#Load the br_netfilter module required for networking.
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

#To allow iptables to see bridged traffic, as required by Kubernetes, we need to set the values of certain fields to 1.

sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

#Apply the new settings without restarting.
sudo sysctl --system

#Install curl.
sudo apt-get install -y apt-transport-https ca-certificates curl

#Add the repository key and the repository for kubeadm, kubectl, kubelet.
mkdir /etc/apt/keyrings

#NOTE: Replace the k8s version in below the URLs 
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

#Update your system and install the 3 Kubernetes modules along with docker.io.
sudo apt update -y
sudo apt install -y kubelet kubeadm kubectl docker.io
sudo apt-mark hold kubelet kubeadm kubectl docker.io

#Configure Containerd
sudo mkdir /etc/containerd
sudo containerd config default > /etc/containerd/config.toml
sudo sed -i 's/            SystemdCgroup = false/            SystemdCgroup = true/' /etc/containerd/config.toml
cat /etc/containerd/config.toml | grep -i systemdcgroup
sudo systemctl restart containerd
sudo systemctl restart kubelet

#To check that containerd is indeed running, use this command:
ps -ef | grep containerd

#Set up the firewall by installing the following rules on the master/both the nodes:
sudo ufw allow 6443/tcp
sudo ufw allow 2379/tcp
sudo ufw allow 2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10251/tcp
sudo ufw allow 10252/tcp
sudo ufw allow 10255/tcp
sudo ufw reload

#And these rules on the worker/both the nodes:
sudo ufw allow 10251/tcp
sudo ufw allow 10255/tcp
sudo ufw reload



#Finally, enable the kubelet service on both systems so we can start it.
sudo systemctl enable kubelet


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
###On Master Node:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Step 3. Setting up the cluster
#With our container runtime and Kubernetes modules installed, we are ready to initialize our Kubernetes cluster.

#Run the following command on the master node to allow Kubernetes to fetch the required images before cluster initialization:
sudo kubeadm config images pull

#Initialize the cluster (CONTROL PLANE)
#sudo kubeadm init --pod-network-cidr=10.244.0.0/16
sudo kubeadm init --pod-network-cidr=10.206.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem

#The initialization may take a few moments to finish. Expect an output similar to the following:

#Your Kubernetes control-plane has initialized successfully!
#To start using your cluster, you need to run the following as a regular user:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#Check the kubeadm config file (Just FYI)
kubectl -n kube-system get cm kubeadm-config -o yaml
#Check the kubelet config file  (Just FYI)
kubectl -n kube-system get cm kubelet-config -o yaml

#You will see a kubeadm join at the end of the output. Copy and save it in some file. We will have to run this command on the worker node to allow it to join the cluster. If you forget to save it, or misplace it, you can also regenerate it using this command:

#sudo kubeadm token create --print-join-command

#Run below to see that the control plane is not ready, because we have not yet installed CNI
kubectl get nodes -o wide

#Deploy a pod network to our cluster. This is required to interconnect the different Kubernetes components.
#Flannel do not support Network Policy and Ingress
	#kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
#Install Calico
	kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/tigera-operator.yaml
	curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.3/manifests/custom-resources.yaml -O
	#Edit the yaml to mention our POD network CIDR
	vi custom-resources.yaml
	kubectl create -f custom-resources.yaml

#Use the get nodes command to verify that our master node is ready.
kubectl get nodes -o wide

#Also check whether all the default pods are running:
kubectl get pods --all-namespaces

#Check whether the DNS lookup is working
#First, create the POD
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "infinity"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
#Second run below to check
kubectl exec -it pod/dnsutils -- nslookup kubernetes.default
kubectl exec -it pod/dnsutils -- nslookup www.google.com

#We are now ready to move to the worker node. Execute the kubeadm join from step 2 on the worker node. You should see an output similar to the following:
#Run below the command in master/control-plane node
#sudo kubeadm token create --print-join-command
#Run the printed command (as below) in the worker node
kubeadm join 10.205.3.150:6443 --token oyotee.2razzjhjc1a9l9n8 \
        --discovery-token-ca-cert-hash sha256:14b26591792fb9043dbe7c78f6575bc13ba8a787069fdf2753f804316ef76bc8
#Label the worker node as worker
kubectl label node k-worker1 node-role.kubernetes.io/worker=worker

kubectl get nodes

#The output should show the worker nodeâ€™s role as follows:

NAME          STATUS   ROLES                  AGE     VERSION
master-node   Ready    control-plane,master   5m12s   v1.24.1
worker-node   Ready    worker                 2m55s   v1.24.1


